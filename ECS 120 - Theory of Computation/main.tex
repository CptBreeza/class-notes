\documentclass[11pt]{article}
\usepackage{lindrew}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows}
\tikzset{
  ->,
  >=stealth,
  node distance=3cm,
  every state/.style={thick, fill=gray!10},
  initial text=$ $,
}
\usepackage{ulem}
\usepackage{pgf-pie}
\usepackage{qtree}
\usepackage{mathtools}

\title{ECS 120: Theory of Computation}
\author{CptBreeza}

\begin{document}

\maketitle

\section{Alphabet, String and Language}

\subsection{Why}

In this course, we model an abstract computing machine with a function
$M : I \mapsto S$, where $I$ stands for \emph{inputs} and $S$ stands for
\emph{solutions}.

It is convenient to \emph{encode} both inputs and solutions as \emph{strings} over
a specific \emph{alphabet}. Consider problems called \emph{decision problems},
where there are only two elements in the solution set: \emph{true} or
\emph{false}, \emph{yes} or \emph{no}, $0$ or $1$. Thus, the inputs are
partitioned by the machine into two classes: those the machine gives a positive answer and those the machine gives a negative answer. We can collect all the
inputs which the machine gives positive answers into a set and we call this set a
language. And we begin our study of the theory of computation with the
definitions of \emph{alphabets}, \emph{strings} and \emph{languages}.

\subsection{Basic Concepts}

First, we define a \emph{language}:

\begin{definition}
A \emph{language} is a set of strings.
\end{definition}

That is pretty straightforward. A language can consist of no strings. A language can consist of an infinite number of strings. There's no requirements.

Next, we define a \emph{string}:
\begin{definition}
A \emph{string} is a finite sequence of characters.
\end{definition}

According to this definition, a string cannot be of infinite length. However, it can be empty. An empty string is typically denoted by $\epsilon$. The characters that make up a string are said to be drawn from an \emph{alphabet}:

\begin{definition}
An \emph{alphabet} is a finite nonempty set.
\end{definition}

We call an element of an alphabet a \emph{symbol}. We do not always specify the underlying alphabet.

\subsection{A Closer Look at Strings} \label{subsec:1.3}

A string has a nonnegative \emph{length}, which is defined as follows:
\begin{definition}
The \emph{length} of a string $A$ is:
\begin{enumerate}
\item $0$, if $A$ is an empty string.
\item $1 + n$, if $A$ is $c \cdot B$ and the length of $B$ is $n$.
\end{enumerate}
\end{definition}

In the definition above, we use capital letters for strings and lowercase letters for symbols.

This definition is an \emph{inductive definition}. After all, a string is nothing but a sequence of symbols (characters). We can always build a new string from an existing one by repeatedly attaching symbols to its head. To build new strings, there is always a starting string available even if we do not have any other string yet - the empty string $\epsilon$.

By thinking of strings as built from other strings, and considering that there is always a starting string $\epsilon$, we can classify strings into two types:
\begin{enumerate}
\item The empty string.
\item Strings that are built from other strings.
\end{enumerate}

Thus, we can simplify the definitions of functions over strings by exploiting this classification, as we did above in the definition of \emph{length}.

Of course, we can build new strings by concatenating existing strings:
\begin{definition}
The concatenation of two strings $A$ and $B$, $A \cdot B$, is:
\begin{enumerate}
\item $B$, if $A$ is $\epsilon$.
\item $a \cdot (A' \cdot B)$, if $A$ is $a \cdot A'$.
\end{enumerate}
\end{definition}

The dot notation $A \cdot B$ for concatenation is not chosen arbitrarily. It is similar to multiplication. It is \emph{associative}, so we can write $A \cdot B \cdot C$ without parentheses. The empty string $\epsilon$ acts like the number 1 in multiplication because, for any string $A$, both $A \cdot \epsilon$ and $\epsilon \cdot A$ are equal to $A$. It is easy to prove that $length(A \cdot B) = length(A) + length(B)$ for any strings $A$ and $B$.

We can also build a new string by reversing an existing one:
\begin{definition}
The reverse of a string $A$, $rev(A)$, is:
\begin{enumerate}
\item $\epsilon$, if $A$ is $\epsilon$.
\item $rev(A') \cdot a$, if $A$ is $a \cdot A'$.
\end{enumerate}
\end{definition}

It is true that for any strings $A$ and $B$, $rev(A \cdot B) = rev(B) \cdot rev(A)$.

\section{Languages, Deterministic Finite Automata}

\subsection{More on Languages}

To work with languages, we introduce an operation called \emph{Kleene star}:
\begin{definition}
The \emph{Kleene star} of a language $L$, denoted by $L^\ast$, is $\{x_1x_2x_3 \dots x_n | x_i \in L\}$.
\end{definition}

This definition is a little bit ambiguous. The ambiguity lies in the suspicious $\dots$. What if, say, $n$
is $0$?

To give a more rigorous definition, first we extend the concatenation operation of strings to languages:
\begin{definition}
The \emph{concatenation} of two languages $L_1$ and $L_2$, $L_1 \cdots L_2$, is $\{x \cdot y | x \in L_1 and y \in L_2\}$.
\end{definition}

And we define the \emph{power} of concatenation of languages:
\begin{definition}
For any nonnegative integer $n$ and any language $L$, $L^n$ is
\begin{enumerate}
\item $\{\epsilon\}$, if $n$ is $0$.
\item $L \cdot L^{n-1}$, if $n$ is greater than $0$.
\end{enumerate}
\end{definition}

We can define the \emph{Kleene star} of a language $L$ by powers of $L$:
\begin{definition}
$L^\ast = \bigcup_{n \in \mathbb{N}}L^n$.
\end{definition}

Now the ambiguity is resolved. According to the definition, it is clear that the empty string $\epsilon$ is
always in $L^\ast$ as a result of the case where $n$ is $0$.

By observing the preceding discussion, we find yet another way to define the \emph{Kleene star}:
\begin{definition}
$L^\ast$ is a set which
\begin{enumerate}
\item has $L$ as its subset.
\item is \emph{closed} under concatenation.
\end{enumerate}
\end{definition}

\subsection{Deterministic Finite Automata}

We might have seen a (finite) state machine before. A \emph{finite state machine (FSM)}, which we call
\emph{Deterministic Finite Automaton (DFA)} in this course, is a facility which
\begin{enumerate}
\item stores a state.
\item accepts inputs.
\item may move (transition) to another state under certain inputs.
\end{enumerate}

It can be used to test if a string has certain \emph{pattern}. For example, consider the DFA shown below:
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state,right of=q0] (q1) {$q_1$};
      \node[state,right of=q1] (q2) {$q_2$};
      \node[state,accepting,right of=q2] (q3) {$q_3$};

      \draw (q0) edge[loop above] node{0}   (q0)
            (q0) edge[above]      node{1}   (q1)
            (q1) edge[loop above] node{0}   (q1)
            (q1) edge[above]      node{1}   (q2)
            (q2) edge[loop above] node{0}   (q2)
            (q2) edge[above]      node{1}   (q3)
            (q3) edge[loop above] node{0,1} (q3);
    \end{tikzpicture}
    \caption{a simple DFA}
    \label{fig:fig1}
\end{figure}

From the diagram, we can infer that the underlying alphabet is $\{0, 1\}$ and this DFA \emph{accepts} strings
that consist of at least three $1$s.

We define a DFA formally as follows:
\begin{definition}
A \emph{Finite Deterministic Automaton}, $M$, is a tuple $\{Q, \Sigma, \delta, q_0, F\}$ where:
\begin{itemize}
\item $Q$ is a finite nonempty set of \emph{states}.
\item $\Sigma$ is the alphabet of the inputs.
\item $\delta: Q \times \Sigma \mapsto Q$ is a transition function which specifies how states are changed over
inputs.
\item $q_0 \in Q$ is the initial state.
\item $F \subseteq Q$ is the set of accept states.
\end{itemize}
\end{definition}

As a DFA \emph{accepts} some strings but reject others, we can partition a language with it. Furthermore,
strings which a DFA $M$ accepts form a language $L$:
\begin{definition}
The set of all strings that a DFA $M$ accepts, $M$, is a language. We denote it by $L(M)$.
\end{definition}

What does it mean for a DFA to \emph{accept} a string? First, we define an auxiliary function $\delta ^ \ast$ - an
extension of the function $\delta$ shown above:
\begin{definition}
The function $\delta ^ \ast : Q \times \Sigma ^ \ast \mapsto Q$ maps a state and a string to another state.

For any state $q$ and string $s$, $\delta ^ \ast (q, s)$ is:
\begin{enumerate}
\item $q$, if $s$ is the empty string $\epsilon$.
\item $\delta ^ \ast( \delta(q, a), s' )$, if $s$ is $a \cdot s'$.
\end{enumerate}
\end{definition}

Then, with this definition, we say that a DFA $M$ accepts a string $s$ iff $\delta ^ \ast(q_0, s) \in F$.

\section{Building DFAs} \label{sec:3}

\subsection{Example 1: Can We Build a Smaller DFA?} \label{subsec:3.1}

What we want: build a DFA accept the language $\{s \in \{a,b\} ^\ast \mid \text{s
does not end with "bbb"} \}$.

How can we build it: instead of thinking about "not end with", first we build a DFA which accepts strings that end with "bbb". And then we "negate" it by turning the
accept states into non-accept states and non-accept states into accept states.
That is pretty straightforward. First, we build the DFA as shown in \ref{fig:fig2}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state,right of=q0] (q1) {$q_1$};
      \node[state,right of=q1] (q2) {$q_2$};
      \node[state,accepting,right of=q2] (q3) {$q_3$};

      \draw (q0) edge[loop,above]           node{a}   (q0)
            (q0) edge[bend left,above]      node{b}   (q1)
            (q1) edge[bend left,above]      node{a}   (q0)
            (q1) edge[bend left,above]      node{b}   (q2)
            (q2) edge[bend left,above]      node{a}   (q0)
            (q2) edge[bend left,above]      node{b}   (q3)
            (q3) edge[bend left,above]      node{a}   (q0)
            (q3) edge[loop,above]           node{b}   (q3);
    \end{tikzpicture}
    \caption{a DFA which accepts all strings that end with "bbb"}
    \label{fig:fig2}
\end{figure}

Then, we negate the states to get the desired DFA:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting] (q0) {$q_0$};
      \node[state,accepting,right of=q0] (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};
      \node[state,right of=q2] (q3) {$q_3$};

      \draw (q0) edge[loop,above]           node{a}   (q0)
            (q0) edge[bend left,above]      node{b}   (q1)
            (q1) edge[bend left,above]      node{a}   (q0)
            (q1) edge[bend left,above]      node{b}   (q2)
            (q2) edge[bend left,above]      node{a}   (q0)
            (q2) edge[bend left,above]      node{b}   (q3)
            (q3) edge[bend left,above]      node{a}   (q0)
            (q3) edge[loop,above]           node{b}   (q3);
    \end{tikzpicture}
    \caption{a DFA which accepts all strings that do not end with "bbb"}
    \label{fig:fig3}
\end{figure}

Can we build a DFA accepting the same language but with fewer states? Or, if stated in another way, is this DFA a \emph{minimal} one? I don't think so. The intuition is
this: since we want to filter (or filter out) all strings that end with "bbb", we
need to count how many "b"s are there right before the inputs are exhausted. And for
DFAs, this count is remembered, or encoded, by states. We need four states to
represent possible counts that can distinguish different situations, where the
string is one "b" from being accepted, two "b"s from being accepted, three "b"s from
being accepted and is already acceptable.

Can we prove it? Surely we can. We prove it by the \emph{Pigeonhole Principle}.

\begin{proof}
We prove this by contradiction. Assume that we \emph{can} build such a DFA $M$
with three states. Now consider following strings:
\begin{align*}
&\epsilon \\
&b \\
&bb \\
&bbb
\end{align*}

Or more compactly, the strings $b^i$ where $0 \leq i \leq 3$. There are four of
them. We feed them into $M$ and we get four states $\delta ^ \ast(q_0, b^i)$.
Since $M$ has three states and there are four states produced, by the
Pigeonhole Principle, at least two of the produced states are the same one.

Let us check it. We pick two strings $b^i$ and $b^I$, where $0 \leq i < I
\leq 3$. We feed them into of $M$. The states produced after that are $\delta ^
\ast(q_0, b^i)$ and $\delta ^ \ast(q_0, b^I)$. But we do not stop here. We then feed
the string $b^{3-I}$. And that produces states $\delta ^ \ast(q_0, b^{i + 3 - I})$
and $\delta ^ \ast(q_0, b^3)$. Are they the same state? The state $\delta ^
\ast(q_0, b^3)$ is an accept state. Since $i < I$, $i + 3 - I$ is strictly less
than $3$. That implies that state $\delta ^ \ast(q_0, b^{i + 3 - I})$ is not an
accept state. These states are distinct.

We have now proved that for any two of the four strings listed above, the states
produced after feeding them into $M$ are distinct. That is to say that $M$ has
at least four distinct states. This contradicts our original hypothesis that
$M$ has three states. So we cannot build such a DFA with three states.
\end{proof}

\subsection{Example 2: Can We Build That DFA At All?}

What we want: build a DFA accepting the language $\{s \in \{a,b\}^\ast \mid a^nb^n
, n \in \mathbb{N}\}$.

How can we build it: I don't think we can build it. As we have seen in the example
in \ref{subsec:3.1}, states of DFAs can remember, or encode, certain information.
But as the name of DFA suggests, there is only a finite amount of information can
be stored by the finite states. To accept the language shown above, however, requires
the machine to remember an indefinite amount of information. That cannot be done
with a finite number of states.

Can we prove it? Surely we can. We prove it by the Pigeonhole Principle.

\begin{proof}
We prove this by contradiction. Assume that we \emph{can} build such a DFA $M$ with
$me$ states. Now consider following strings:
\begin{align*}
&\epsilon \\
&a^mb \\
&a^mb^2 \\
&\dots \\
&a^mb^m
\end{align*}
Or more compactly, the strings $a^mb^i$ where $0 \leq i \leq m$. There are $m + 1$ of
them. We feed them into $M$ and we get $m + 1$ states $\delta ^ \ast(q_0, a^mb^i)$.
Since $M$ has $m$ states and there are $m + 1$ states produced, by the Pigeonhole
Principle, at least two of the produced states are the same one.

Let us check it. We pick two strings $b^i$ and $b^I$, where $0 \leq i < I \leq m$. We
feed them into $M$. The states produced after that are $\delta ^ \ast(q_0, a^mb^i)$
and $\delta ^ \ast(q_0, a^mb^I)$. But we do not stop here. We then feed the string
$b^{m-I}$. And that produces states $\delta ^ \ast(q_0, b^{i + m - I}$ and
$\delta ^ \ast(q_0, b^m)$. Are they the same state? The state
$\delta ^ \ast(q_0, b^m)$ is an accept state. Since $i < I$, $i + m - I$ is
strictly less than $m$. That implies that the state
$\delta ^ \ast(q_0, b^{i + m - I}$ is not an accept state. These states are
distinct.

We have now proved that for any two of the four strings listed above, the states
produced after feeding them into $M$ are distinct. That is to say that $M$ has at
least $m + 1$ distinct states. This contradicts our original hypothesis that $M$ has
$m$ states. So we cannot build such a DFA.
\end{proof}

There are languages which cannot be accept by any DFA. And the reason of this is that
DFA is not powerful enough.

\subsection{Can We Build new DFAs Systematically From Exising Ones?} \label{subsec:3.3}

What we want: As discussed in \ref{subsec:1.3}, we can build new strings from
existing ones by concatenation and reversal. Similarly, we want some systematical
methods to build DFAs from existing ones.

How we can build it: It is not easy to compose DFAs. While we developed a connection
between languages and DFAs. Languages are sets. We instead consider following
questions:
Assume there are DFAs that can accept $L_1$ and $L_2$.
\begin{enumerate}
\item Is there a DFA that accept $\overline{L_1}$?
\item Is there a DFA that accept $L_1 \cup L_2$?
\item Is there a DFA that accept $L_1 \cap L_2$?
\end{enumerate}

\subsubsection{Complement}

As we see in the example in \ref{subsec:3.1}, where we build a DFA and then negate
it. It is obvious that we can always build a new DFA from an existing one by negating
all its states.

\subsubsection{Union} \label{subsubsec:3.3.2}

It is possible to build a DFA accepting $L_1 \cup L_2$.

Imagine that $L_1$ is at our left hand and $L_2$ is at our right hand. As the input
symbols are fed, we move hands following the transitions respectively in the two DFAs. After all inputs are fed, each of our hands would point to a state. If either
the final states pointed is an accept state, then we say that the new DFA accept
the input. And this DFA accepts $L_1 \cup L_2$.

Let us construct it formally.
\begin{definition}
Assume that there are two machines $M_1=\{Q_1,\Sigma,\delta_1,q_1,F_1\}$ and
$M_2 = \{Q_2, \Sigma,\delta_2,q_2,F_2\}$. They accepts $L_1$ and $L_2$ respectively.
The machine $M = \{Q,\Sigma,\delta,q_0,F\}$ accepting $L_1 \cup L_2$ can be
constructed as follows:
\begin{enumerate}
\item $Q = Q_1 \times Q_2$
\item $\Sigma$ = $\Sigma$
\item $\delta(q, s) = (\delta_1(q_1, s), \delta_2(q_2, s))$ where $q = (q_1, q_2)$
\item $q_0 = (q_1, q_2)$
\item $F = \{q \in Q \mid q_1 \in F_1 \vee q_2 \in F_2\}$
\end{enumerate}
\end{definition}

The construction defined above is often called a \emph{product machine}, since the
number of states of it is the product of the two building DFAs.

\subsubsection{Intersection}

It is possible to build a DFA accepting $L_1 \cap L_2$.

In fact, we can employ the same construction that we build in \ref{subsubsec:3.3.2}
- the product machine - to build this DFA. All is same but $F$. In this case, $F$
should be $\{q \in Q \mid q_1 \in F_1 \wedge q_2 \in F2\}$.

There is another way to build this DFA by exploiting the \emph{De Morgan's Law}.
That is $L_1 \cap L_2 = \overline{\overline{L_1} \cup \overline{L_2}}$.

\section{Nondeterministic Finite Automata} \label{sec:4}

As shown in \ref{subsec:3.3}, given $L_1 = L(M_1)$ and $L_2 = L(M_2)$, there exists
$M$ that accepts
\begin{enumerate}
\item $\overline{L_1}$
\item $L_1 \cup L_2$
\item $L_1 \cap L_2$
\end{enumerate}

We have discussed two operations $L_1 \cdot L_2$ and $L^\ast$ but they are not on
this list. Can we build a DFA that accepts $L_1 \cdot L_2$ or $L^\ast$?

\subsection{Can We Build new DFAs Systematically From Existing Ones? Cont.}

\subsubsection{Concatenation} \label{subsubsec:4.1.1}

For a machine to accept $L(M_1) \cdot L(M_2)$, we need to

\begin{enumerate}
\item wire up the accept states of $M_1$ to the initial state of $M_2$
\item turn the accept states of $M_1$ to nonaccept states
\end{enumerate}

Then, for any string to reach the accept states of the resultant machine, it must go
through first $M_1$ and then $M_2$. This is exactly what we want for a machine to
accept $L_1 \cdot L_2$.

Here is an example. $M_1$ accepts $L_1$, which is the language of all strings that start
with "bb":

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial]               (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q0) edge[loop,above]           node{a}   (q0)
            (q0) edge[above]                node{b}   (q1)
            (q1) edge[loop,above]           node{a}   (q1)
            (q1) edge[above]                node{b}   (q2)
            (q2) edge[loop,above]           node{a,b} (q2);
    \end{tikzpicture}
    \caption{a DFA which accepts all strings that start with "bb"}
    \label{fig:fig4}
\end{figure}

$M_2$ accepts $L_2$, which is the language of all strings that end with "aa":

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial]               (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q0) edge[loop,above]           node{b} (q0)
            (q0) edge[bend left,above]      node{a} (q1)
            (q1) edge[bend left,above]      node{b} (q0)
            (q1) edge[above]                node{a} (q2)
            (q2) edge[loop,above]           node{a} (q2)
            (q2) edge[bend left,above]      node{b} (q0);
    \end{tikzpicture}
    \caption{a DFA which accepts all strings that end with "aa"}
    \label{fig:fig5}
\end{figure}

Below is a DFA which accepts $L_1 \cdot L_2$:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial]               (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,right of=q1]           (q2) {$q_2$};
      \node[state,right of=q2]           (q3) {$q_3$};
      \node[state,right of=q3]           (q4) {$q_4$};
      \node[state,accepting,right of=q4] (q5) {$q_5$};

      \draw (q0) edge[loop,above]           node{a}   (q0)
            (q0) edge[above]                node{b}   (q1)
            (q1) edge[loop,above]           node{a}   (q1)
            (q1) edge[above]                node{b}   (q2)
            (q2) edge[above]                node{?}   (q3)
            (q2) edge[loop,above]           node{a,b} (q3)
            (q3) edge[loop,above]           node{b}   (q3)
            (q3) edge[bend left,above]      node{a}   (q4)
            (q4) edge[bend left,above]      node{b}   (q3)
            (q4) edge[above]                node{a}   (q5)
            (q5) edge[loop,above]           node{a}   (q5)
            (q5) edge[bend left,above]      node{b}   (q3);
    \end{tikzpicture}
    \caption{a \sout{DFA} which accepts all strings that start with "bb" and end with "aa"}
    \label{fig:fig6}
\end{figure}

It seems to be working fine, except the transition from $q_2$ to $q_3$.
As the question mark above the transition line in the diagram suggests,
there is no symbol from the alphabet (which is, inferred from context,
$\{a,b\}$) that could do the job.

So, why not introduce a special purpose symbol for convenience? To be
specific, we can introduce a symbol, $\epsilon$, to indicate that the
transition can be made with no input symbol at all. The notation is
not a coincidence. In previous discussion, we use $\epsilon$ to denote
the empty string. Here, $\epsilon$ means \emph{nothing},
\emph{empty symbol}.

Let us fix figure \ref{fig:fig6} by putting replacing the "?" with
$\epsilon$:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial]               (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,right of=q1]           (q2) {$q_2$};
      \node[state,right of=q2]           (q3) {$q_3$};
      \node[state,right of=q3]           (q4) {$q_4$};
      \node[state,accepting,right of=q4] (q5) {$q_5$};

      \draw (q0) edge[loop,above]           node{a}   (q0)
            (q0) edge[above]                node{b}   (q1)
            (q1) edge[loop,above]           node{a}   (q1)
            (q1) edge[above]                node{b}   (q2)
            (q2) edge[above]                node{$\epsilon$}   (q3)
            (q2) edge[loop,above]           node{a,b} (q3)
            (q3) edge[loop,above]           node{b}   (q3)
            (q3) edge[bend left,above]      node{a}   (q4)
            (q4) edge[bend left,above]      node{b}   (q3)
            (q4) edge[above]                node{a}   (q5)
            (q5) edge[loop,above]           node{a}   (q5)
            (q5) edge[bend left,above]      node{b}   (q3);
    \end{tikzpicture}
    \caption{an NFA which accepts all strings that start with "bb" and end with "aa" fixed}
    \label{fig:fig7}
\end{figure}

This machine, with a transition labeled by $\epsilon$, is no longer
a DFA. If the machine is at state $q_2$, and we feed an $a$ into it,
it can stay at $q_2$ or transition to $q_4$. Why $q_4$? The machine
is not \emph{stable}, or not deterministic, at $q_2$ in the presence
of the $\epsilon$ transition, in the sense that it is actually at
both $q_2$ and $q_3$. If it is at $q_2$, it would transition to
$q_2$. If it is at $q_3$, it would transition to $q_4$.

The key difference between this machine and a DFA is that with an
input there is only one path possible in a DFA, but many paths in
this machine. By this feature, we call this machine a
\emph{nondeterministic finite automaton (NFA)}.

What is for an NFA to accept a string? For a DFA, it accepts a string
as long as \emph{the} path leads to an accept state. For a NFA, since
there are many paths possible, it accepts a string as long as there is
\emph{a} path that leads to an accept state.

Now, We have shown that with the introduction of the $\epsilon$
transition, we can build an NFA $M$ out of DFAs $M_1$ and $M_2$
that accepts $L = L_1 \cdot L_2$. This is not what we are asking
for. We are asking for a DFA but instead got an NFA. In latter
discussions, we will show that we can build a DFA that accepts
exactly the same language out of an NFA. And that is to say
that we can build a DFA that accepts $L_1 \cdot L_2$.

\subsubsection{Kleene Star}

To build a DFA or an NFA that accepts $L^\ast$, we employ a strategy
similar to the one we used in section \ref{subsubsec:4.1.1}. That is,
for $L^\ast$, we

\begin{enumerate}
\item wire up the accept states of $M$ to its own initial state
\item turn the initial state to an accept state
\end{enumerate}

Here is an example. $L$ is the language of all strings that start with "bb".
$L^\ast$ contains all strings that start with "bb" and the empty string $\epsilon$.
Following the instruction above, we can bulid an NFA for $L^\ast$. The diagram
of $M$ is shown in figure \ref{fig:fig8}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]     (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q0) edge[loop,above]           node{a}   (q0)
            (q0) edge[above]                node{b}   (q1)
            (q1) edge[loop,above]           node{a}   (q1)
            (q1) edge[above]                node{b}   (q2)
            (q2) edge[loop,above]           node{a,b} (q2)
            (q2) edge[bend left,above]           node{$\epsilon$} (q0);
    \end{tikzpicture}
    \caption{an NFA that is supposed to accept $L^\ast$ where $L$ is the language
    of all strings that start with "bb"}
    \label{fig:fig8}
\end{figure}

It almost works, except that it is problematic to turn the initial state to an
accept state. Consider the example of $M$ , shown in figure \ref{fig:fig9},
that accepts $L^\ast$ where $L$ is the language of all strings that end with
"aa". If we turn $q_0$ to an accept state, a string like "baab" would be
accepted by $M$. This is certainly not what we want.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]     (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q0) edge[loop,above]           node{b} (q0)
            (q0) edge[bend left,above]      node{a} (q1)
            (q1) edge[bend left,above]      node{b} (q0)
            (q1) edge[above]                node{a} (q2)
            (q2) edge[loop,above]           node{a} (q2)
            (q2) edge[bend left,above]      node{b,$\epsilon$} (q0);
    \end{tikzpicture}
    \caption{an NFA that is supposed to accept $L^\ast$ where $L$ is the language
    of all strings that end with "aa"}
    \label{fig:fig9}
\end{figure}

To fix things up, we need an extra state, which is to be the new initial state.
And the accept states should wire up to this extra state. To be specific, we
need to:
\begin{enumerate}
\item add a new state and make it the new initial state and an accept state
\item add a $\epsilon$ transition from the new initial state to the original
initial state
\item add $\epsilon$ transitions from accept states to the new initial state
\end{enumerate}

$M$ fixed is shown below in figure \ref{fig:fig10}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]     (q)  {$q$};
      \node[state,initial,right of=q]    (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q)  edge[above]                node{$\epsilon$} (q0)
            (q0) edge[loop,above]           node{b} (q0)
            (q0) edge[bend left,above]      node{a} (q1)
            (q1) edge[bend left,above]      node{b} (q0)
            (q1) edge[above]                node{a} (q2)
            (q2) edge[loop,above]           node{a} (q2)
            (q2) edge[bend left,above]      node{b} (q0)
            (q2) edge[bend left,above]      node{$\epsilon$} (q);
    \end{tikzpicture}
    \caption{an NFA that accepts $L^\ast$ where $L$ is the language of all
    strings that end with "aa"}
    \label{fig:fig10}
\end{figure}

Now, we have shown that we can build an NFA $M'$ that accepts $L^\ast$ out of
a DFA $M$. Again, this is not what we are asking for. We are asking for a DFA
but instead got an NFA.

\subsection{Defining Nondeterministic Finite Automata}

In fact, in addition to the $\epsilon$ transition, there are two more features
of NFA:
\begin{enumerate}
\item we do not have to exhaust all symbols of the alphabet for outgoing
transitions of a state
\item there can be more than one possible transitions from one state under an input
\end{enumerate}

These two features can be demonstrated in the following example:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]     (q)  {$q$};
      \node[state,initial,right of=q]    (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q)  edge[above]                node{$\epsilon$} (q0)
            (q0) edge[loop,above]           node{a,b} (q0)
            (q0) edge[bend left,above]      node{a} (q1)
            (q1) edge[above]                node{a} (q2)
            (q2) edge[loop,above]           node{a} (q2)
            (q2) edge[bend left,above]      node{$\epsilon$} (q);
    \end{tikzpicture}
    \caption{an NFA that accepts $L^\ast$ where $L$ is the language of all
    strings that end with "aa"}
    \label{fig:fig11}
\end{figure}

As we can see, the alphabet in this example is $\{a,b\}$. There are no outgoing
transitions from $q_1$ labeled $b$. And from $q_0$, there are two outgoing
transitions labeled $a$, one to $q_0$ and another to $q_1$.

\subsubsection{NFA can be non-exhaustive}

If a symbol is fed, and there are no transitions outgoing labeled with it,
we might think that the machine transitions to a "dead" state. It is a trap
where there is no input can take the machine out of it. Below is an example
of figure \ref{fig:fig11} with an explicit "dead" state.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]     (q)  {$q$};
      \node[state,initial,right of=q]    (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};
      \node[state,below right of=q0]     (dead) {dead};

      \draw (q)  edge[above]               node{$\epsilon$} (q0)
            (q0) edge[loop,above]          node{a,b} (q0)
            (q0) edge[bend left,above]     node{a} (q1)
            (q1) edge[above]               node{a} (q2)
            (q2) edge[loop,above]          node{a} (q2)
            (q2) edge[bend left,above]     node{$\epsilon$} (q)
            (q)  edge[bend right,above]    node{a,b} (dead)
            (q1) edge[bend left,above]     node{b} (dead)
            (q2) edge[bend left,above]     node{b} (dead)
            (dead) edge[loop,above]        node{a,b} (dead);
    \end{tikzpicture}
    \caption{an NFA with an explicit "dead" state}
    \label{fig:fig12}
\end{figure}

\subsubsection{NFA Transitions From One State to Several States}

With $\epsilon$ transitions and the ability to draw several outgoing
transitions from a state of a label, an NFA transitions from one state
to several states.

Here is an example. Let us find out all transitions in the example:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,right of=q]    (q0) {$q_0$};
      \node[state,right of=q0]           (q1) {$q_1$};
      \node[state,accepting,right of=q1] (q2) {$q_2$};

      \draw (q0) edge[loop,above]           node{a,b} (q0)
            (q0) edge[bend left,above]      node{a} (q1)
            (q1) edge[above]                node{a} (q2)
            (q1) edge[bend left,above]      node{a} (q0)
            (q2) edge[loop,above]           node{a} (q2);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig13}
\end{figure}

\begin{align*}
&\delta(q_0, a) = \{q_0,q_1\} \\
&\delta(q_0, b) = \{q_0\} \\
&\delta(q_1, a) = \{q_0,q_2\} \\
&\delta(q_1, b) = \emptyset \\
&\delta(q_2, a) = \{q_1\} \\
&\delta(q_2, b) = \emptyset
\end{align*}

\subsubsection{Formal Definition}

After examining the behaviours, we can finally formally define a NFA:
\begin{definition}
A \emph{Nondeterministic Finite Automaton}, $M$, is a tuple $\{Q,\Sigma,\delta,q_0,F\}$ where:
\begin{itemize}
\item $Q$ is a finite nonempty set of states
\item $\Sigma$ is the alphabet of the inputs
\item $\delta : Q \times (\Sigma \cup \{\epsilon\}) \mapsto \mathcal{P}(Q)$
\item $q_0 \in Q$ is the initial state
\item $F \subseteq Q$ is the set of accept states
\end{itemize}
\end{definition}

And what does it mean for an NFA to accept a string:
\begin{definition}
An NFA $M = \{Q,\Sigma,\delta,q_0,F\}$ accepts a string $s$ \\
if there exists
$a_1,a_2,\dots,a_n \in (\Sigma \cup \{\epsilon\})$ and
$q_q,q_2,\dots,q_n \in Q$ where \\
$s = a_1 a_2 \dots a_n$ and \\
$q_i = \delta(q_{i-1},a_i)$ for all $1 \leq i \leq n$ and \\
$q_n \in F$.
\end{definition}

\section{DFA/NFA equivalence, Regular Language and Regular Expression}

\subsection{Build a DFA out of an NFA by Grouping States} \label{subsec:5.1}

Now it is time to show that every NFA can be turned into a DFA. Let us do this by first
show an example. Consider an NFA $M$ as shown below:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]  (q0) {0};
      \node[state,below right of=q0]  (q1) {1};
      \node[state,below of=q0]        (q2) {2};

      \draw (q0) edge[above]      node{a} (q1)
            (q1) edge[loop,above] node{b} (q1)
            (q1) edge[above]      node{b} (q2)
            (q2) edge[left]       node{a} (q0);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig14}
\end{figure}

We will draw a table of all transitions of $M$. But this time we do it in a slightly
different way. First, instead of $0$, we use $\{0\}$ as the initial state. Then, we
figure out which set of states it transitions to under different inputs. For each
set of states that is transitioned to, we use it as a starting point to continue
chasing next sets of states. The table of transitions built in this new approach is
shown below:

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
set of states & $a$ & $b$ \\
\hline
$\{0\}$ & $\{1\}$ & $\emptyset$ \\
\hline
$\{1\}$ & $\emptyset$ & $\{1,2\}$ \\
\hline
$\{1,2\}$ & $\{0\}$ & $\{1,2\}$ \\
\hline
$\emptyset$ & $\emptyset$ & $\emptyset$ \\
\hline
\end{tabular}
\caption{table of transitions of sets of states of $M$}
\end{table}

Look at this table, if we take these sets of states as states, the table would be
describing a $\delta$ function of some DFA. Why not build a DFA by this? But before
building the DFA, there are two more questions:
\begin{enumerate}
\item what is the initial state?
\item what are the accept states?
\end{enumerate}

The answers to these questions are:
\begin{enumerate}
\item $\{q_0\}$ where $q_0$ is the initial state of the original NFA is the
initial state of the new DFA. In this example, $\{0\}$.
\item any state which has an accept state of the original NFA as its element
is an accept state. In this example, again $\{0\}$.
\end{enumerate}

And finally we can build the DFA. The DFA is shown in figure \ref{fig:fig15}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]  (q0) {$\{0\}$};
      \node[state,right of=q0]        (q1) {$\{1\}$};
      \node[state,right of=q1]        (q2) {$\{1,2\}$};
      \node[state,above of=q1]        (q3) {$\emptyset$};

      \draw (q0) edge[above]      node{a} (q1)
            (q0) edge[left]      node{b} (q3)
            (q1) edge[left]      node{a} (q3)
            (q1) edge[above]      node{b} (q2)
            (q2) edge[bend left,above]      node{a} (q0)
            (q2) edge[loop,above] node{b} (q2)
            (q3) edge[loop,above] node{a,b} (q3);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig15}
\end{figure}

\subsection{Build a DFA out of an NFA by Chasing Down $\epsilon$ Transitions}

For simplicity, there are no $\epsilon$ transitions in the example in
\ref{subsec:5.1}. It takes a little more effort to deal with them. And let us
do it now. The figure below shows a slice of some NFA:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial]  (q0) {0};
      \node[state,accepting,right of=q0]        (q1) {1};
      \node[state,right of=q1]        (q2) {2};
      \node[state,right of=q2]        (q3) {3};

      \draw (q0) edge[above]     node{$\epsilon$} (q1)
            (q1) edge[above]     node{a} (q2)
            (q2) edge[above]     node{$\epsilon$} (q3);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig16}
\end{figure}

It takes three steps to eliminate the $\epsilon$ transitions. First, we add a
new transition to \emph{bypass} the $\epsilon$ transitions. The result is shown
is figure \ref{fig:fig16}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial]  (q0) {0};
      \node[state,accepting,right of=q0]        (q1) {1};
      \node[state,right of=q1]        (q2) {2};
      \node[state,right of=q2]        (q3) {3};

      \draw (q0) edge[above]     node{$\epsilon$} (q1)
            (q1) edge[above]     node{a} (q2)
            (q2) edge[above]     node{$\epsilon$} (q3)
            (q0) edge[bend left,above]     node{a} (q3);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig17}
\end{figure}

Then, if the $\epsilon$ transition points to an accept state, we need to turn
the start state of the $\epsilon$ transition to an accept state. The result is
shown in figure \ref{fig:fig17}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]  (q0) {0};
      \node[state,accepting,right of=q0]        (q1) {1};
      \node[state,right of=q1]        (q2) {2};
      \node[state,right of=q2]        (q3) {3};

      \draw (q0) edge[above]     node{$\epsilon$} (q1)
            (q1) edge[above]     node{a} (q2)
            (q2) edge[above]     node{$\epsilon$} (q3)
            (q0) edge[bend left,above]     node{a} (q3);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig18}
\end{figure}

Finally, we can remove $\epsilon$ transtions safely. The result is shown in
figure \ref{fig:fig19}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting]  (q0) {0};
      \node[state,accepting,right of=q0]        (q1) {1};
      \node[state,right of=q1]        (q2) {2};
      \node[state,right of=q2]        (q3) {3};

      \draw (q1) edge[above]     node{a} (q2)
            (q0) edge[bend left,above]     node{a} (q3);
    \end{tikzpicture}
    \caption{}
    \label{fig:fig19}
\end{figure}

\subsection{Regular Language and Regular Expression}

We have seen so much about machines. Now we turn to languages and see what
languages associated with DFAs/NFAs are like.

We define a class of languages, called \emph{regular language}:
\begin{definition}
Fix the alphabet $\Sigma$. A language, $L$, is a \emph{regular language},
if either of the following is true:
\begin{enumerate}
\item $L = \emptyset$
\item $L = \{\epsilon\}$
\item $L = \{a\}$ where $a \in \Sigma$
\item $L = L_1 \cup L_2$ where $L_1$ and $L_2$ are regular languages
\item $L = L_1 \cdot L_2$ where $L_1$ and $L_2$ are regular languages
\item $L = L_1^\ast$ where $L_1$ is a regular language
\end{enumerate}
\end{definition}

It is easy to see, from the definition, that regular languages are closed
under \emph{union}, \emph{concatenation} and \emph{Kleene star}. And the
that we define it remind us of the ways to build new DFAs out of existing
ones as discussed in previous sections. There is, indeed, a connection
between regular languages and DFAs/NFAs. We will show this in next section.

It is convenient to develop a notation for regular languages, which we
call \emph{regular expressions}. We define them formally in definition
\ref{def:def19}. The regular language which is denoted by a regular
language $s$ is denoted by $L(s)$.

After the formal definition of regular languages, we specify what regular
language a regular expression \emph{matches}. Assume regular expression
$s_1$ matches $L_1$ and $s_2$ matches $L_2$, then:
\begin{enumerate}
\item $L(\epsilon) = \emptyset$
\item $L("\$\{a\}")$ = $\{a\}$
\item $L("\epsilon")$ = $\{\epsilon\}$
\item $L("(\$\{s_1\} \cup \$\{s_2\})") = L_1 \cup L_2$
\item $L("(\$\{s_1\} \cdot \$\{s_2\})") = L_1 \cdot L_2$
\item $L("(\$\{s_1\}^\ast)") = L_1 ^ \ast$
\end{enumerate}

\begin{definition} \label{def:def19}
Fix the alphabet $\Sigma$. A string $s$ over the alphabet
$\Sigma \cup \{\epsilon,(,),\cup,\cdot,\ast\}$ is a \emph{regular
expression}, if either of the following is true:
\begin{enumerate}
\item $s = \epsilon$
\item $s = "\$\{a\}"$ for some symbol $a$ in $\Sigma$
\item $s = "\epsilon"$
\item $s = "(\$\{s_1\} \cup \$\{s_2\})"$ where $s_1$ and $s_2$ are regular
expressions
\item $s = "(\$\{s_1\} \cdot \$\{s_2\})"$ where $s_1$ and $s_2$ are regular
expressions
\item $s = "(\$\{s_1\}^\ast)"$ where $s_1$ is a regular expressions
\end{enumerate}
\end{definition}

\section{DFA/NFA - Regular Language Equivalence and a Technique to Minimize a DFA}

We showed the equivalence between DFAs and NFAs. And we defined a class of
languages which we call the regular languages. In this section, we are going
to show the connection between DFAs/NFAs and regular languages. That is, the
class of languages that DFAs/NFAs accepts is exactly regular languages. There
is an equivalence between DFAs/NFAs and regular languages. We show it by
working in two directions. First, we show that for every regular expression,
we can build a DFA/NFA accepting exactly the regular language represented by
it. Then, we show that for every DFA/NFA, we can derive a regular expression
representing the language that it accepts.

\subsection{Build a DFA/NFA from a Regular Expression}

The easiest way to build a DFA/NFA from a regular expression is following
the definition of regular expressions. The base cases are shown in figure
\ref{fig:fig20} through figure \ref{fig:fig22}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {};

      \draw (q0) edge[loop,above] node{any character} (q0);
    \end{tikzpicture}
    \caption{$\emptyset$}
    \label{fig:fig20}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial,accepting] (q0) {};
      \node[state,right of=q0,xshift=2cm] (q1) {};

      \draw (q0) edge[above] node{any character} (q1)
            (q1) edge[loop,above] node{any character} (q1);
    \end{tikzpicture}
    \caption{$"\epsilon"$}
    \label{fig:fig21}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {};
      \node[state,accepting,right of=q0] (q1) {};
      \node[state,right of=q1,xshift=2cm] (q2) {};

      \draw (q0) edge[above] node{$a$} (q1)
            (q1) edge[above] node{any character} (q2)
            (q2) edge[loop,above] node{any character} (q2)
            (q0) edge[bend right,above] node{any character but $a$} (q2);
    \end{tikzpicture}
    \caption{$"a"$}
    \label{fig:fig22}
\end{figure}

For each inductive case in the definition of regular expressions, there is
a corresponding construction of DFAs/NFAs which we discussed in detail in
section \ref{sec:3} and section \ref{sec:4}. To be specific, in the case of
$s = s_1 \cup s_2$, we use the \emph{product construction} to combine two
DFAs together. In the case of $s = s_1 \cdot s_2$, first we wire up $M_1$
and $M_2$ in the way that there are $\epsilon$ transitions connecting the
accept states of $M_1$ and the initial state of $M_2$, then we turn the
accept states of $M_1$ to nonaccept states. In the case of $s^\ast$, first
we add a new state and make it the initial state and a accept state, add
an $\epsilon$ transition from the new initial state to the original initial
state, and add $\epsilon$ transitions from the original accept states to
the original initial state.

\subsection{Derive a Regular Expression from an NFA}

To derive a regular expression from an NFA, we eliminates states from it.

First, we add a new initial state and a $\epsilon$ transition from it to
the original initial state.

Then, we add an accept state and $\epsilon$ transitions from the original
accept states to it. And we turn the original accept states to nonaccept
states.

After that, we pick a state from the original NFA and look at all paths
through it. A path should look like that shown below:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state,right of=q0] (q1) {$q_1$};
      \node[state,right of=q1] (q2) {$q_2$};

      \draw (q0) edge[above] node{a} (q1)
            (q1) edge[loop,above] node{b} (q2)
            (q1) edge[above] node{c} (q2);
    \end{tikzpicture}
    \caption{a typical path through $q_1$}
    \label{fig:fig23}
\end{figure}

For this path, to eliminate $q_1$, we add a new transition from $q_0$ to
$q_2$, labelled by $a \cdot b^\ast c$, and remove the original transition
from $q_0$ to $q_1$ and that from $q_1$ to $q_2$. The resultant figure is
shown below:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state, right of=q0] (q1) {$q_1$};
      \node[state, right of=q1] (q2) {$q_2$};

      \draw (q0) edge[bend left,above] node{$a \cdot b^\ast c$} (q2);
    \end{tikzpicture}
    \caption{figure \ref{fig:fig23} after setting up the new transition}
    \label{fig:fig24}
\end{figure}

It should be noted that this is just one of the paths through $q_1$.
Generally, there could be many paths through $q_1$. We need to set up a
transition like this for each path. After removing original transitions
for every path, we can safely eliminate $q_1$. This process might end up
with a situation where there are two or more outgoing transitions from a
state, to one another state. An example is shown in figure \ref{fig:fig25}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state, right of=q0] (q2) {$q_2$};

      \draw (q0) edge[bend left,above] node{$s_1$} (q2)
            (q0) edge[bend right,below] node{$s_2$} (q2);
    \end{tikzpicture}
    \caption{a situation where there are two transitions from $q_0$ to $q_2$}
    \label{fig:fig25}
\end{figure}

In this case, we combine those outgoing transitions to a new transition
labeled by $"\$\{s_1\}\cup\$\{s_2\}"$.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state, right of=q0,xshift=1cm] (q2) {$q_2$};

      \draw (q0) edge[above] node{$"\$\{s_1\}\cup\$\{s_2\}"$} (q2);
    \end{tikzpicture}
    \caption{figure \ref{fig:fig25} after combining transitions}
    \label{fig:fig26}
\end{figure}

The new machine produced by the method shown above is no longer a DFA
or NFA, since the transitions are now labeled by regular expressions
instead of symbols drawn from the alphabet. We call it a \emph{generalized
nondeterministic finite automaton}. As we can imagine, after all original
states are eliminated, there would be two states and a transition between
them left. The label of the transition is the regular expression
representing the language which the original DFA/NFA accepts.

\subsection{A Technique to Minimize a DFA} \label{subsec:6.3}

To minimize a DFA, we find \emph{equivalent states} and combine them.
What does it mean for states to be equivalent? For a DFA $M$, imagine
two states $q_0$ and $q_1$ of it. If $\delta(q_0, x) = \delta(q_1, x)$
holds for any string $x$, it makes no difference whether $M$ is in $q_0$
or $q_1$. So we can eliminate one of them and migrate all its ingoing
transitions to the other one.

This is better explained in an example. Consider the DFA shown below:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state,right of=q0] (q2) {$q_2$};
      \node[state,accepting,below of=q2] (q1) {$q_1$};
      \node[state,right of=q2] (q3) {$q_3$};

      \draw (q0) edge[above] node{$0$} (q2)
            (q2) edge[loop,above] node{$0$} (q2)
            (q2) edge[bend left,above] node{$1$} (q3)
            (q3) edge[bend left,above] node{$0$} (q2)
            (q3) edge[bend left,above] node{$1$} (q1)
            (q1) edge[loop,above] node{$0,1$} (q1)
            (q1) edge[bend left,above] node{$1$} (q0);
    \end{tikzpicture}
    \caption{figure \ref{fig:fig25} after combining transitions}
    \label{fig:fig27}
\end{figure}

Which states should be considered equivalent? $q_1$ is not equivalent
to any other states since it is an accept state. $q_0$ is not
equivalent to $q_2$ because input $1$ takes $q_0$ to $q_2$ but takes
$q_2$ to $q_3$. $q_2$ is not equivalent to $q_3$ because input $1$
takes $q_2$ to $q_3$ but takes $q_3$ to $q_1$. $q_0$ is equivalent to
$q_3$ because both inputs $0$ and $1$ take them to the same states.
We decide to eliminate $q_3$. To do so, first we migrate all its
ingoing transitions to $q_0$. There is one ingoing transition from $q_2$.
So we add a new transition from $q_2$ to $q_0$ labeled by $1$. And we
remove $q_3$ together with all transitions associated with it. This
yields the DFA shown below:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state,right of=q0] (q2) {$q_2$};
      \node[state,accepting,below of=q2] (q1) {$q_1$};

      \draw (q0) edge[bend left,above] node{$0$} (q2)
            (q2) edge[loop,above] node{$0$} (q2)
            (q2) edge[bend left,above] node{$1$} (q0)
            (q1) edge[loop,above] node{$0,1$} (q1)
            (q1) edge[bend left,above] node{$1$} (q0);
    \end{tikzpicture}
    \caption{minimized version of figure \ref{fig:fig27}}
    \label{fig:fig28}
\end{figure}

\section{Two Theorems about Regular Languages}

In this section, we will introduce two theorems about regular languages.
Both of them can be used to proof the (ir)regularity of a language.

\subsection{Myhill-Nerode Theorem}

$\Sigma^\ast$ can be partitioned into two \emph{equivalence classes} by
a DFA $M$, namely a class of strings accepted by $M$ and a class of
those not. With $M$, assuming $|Q_M| = n$, $\Sigma^\ast$ can also be
partitioned into $n$ classes, by the final states reached after the string
is fed into $M$. Obviously, the latter partition is a refinement of the
first one. The big $L(M)$ class in the first partition is the union of
all classes of the second one where the state is an accept state.

Now we suggest a partition that is finer than the first one while coarser
than the second one. Recall from \ref{subsec:6.3} that we have defined a
equivalence relation upon strings. We give a formal definition here:

\begin{definition} \label{def:def20}
Fix the alphabet $\Sigma$, two strings $x$ and $y$ are said to be
\emph{indistinguishable with respect to $L$}, which we denote $x \equiv_L y$,
if for every $z \in \Sigma ^\ast$, $xz \in L$ if and only if $yz \in L$.
\end{definition}

For a DFA $M$, what the definition \ref{def:def20} says is basically that
for any two strings $x$ and $y$, $x \equiv_{L_M} y$ if and only if we feed
them into $M$ respectively and then feed any string $z$, either both or none
of them reach accept states. And two states $q_1$ and $q_2$ can be considered
equivalent in the same manner that, if fed with any string $z$, both or none
of them transition to accept states.

For example, consider the language $\{s \in \Sigma^\ast \mid \text{s ends
with "bbb"}\}$. For this language, "b" and "bb" are not equivalent, since
after appending $z =$ "b" to them, the first reaches a nonaccept state
and the second reaches an accept state. "abb" and "abbabb" are equivalent.

The new partition we want to establish is that of classes of equivalent
strings with respect to $L(M)$. It is finer than the first partition,
because all nonaccepted strings belong to one single class in the first
partition, but may belong to several classes in the new partition. It is
coarser than the second partition, in which every class corresponds to a
state of $M$, whereas in the new partition the equivalent states are
combined. An example of three partitions is shown below in figure
\ref{fig:fig29}.

\begin{figure}
\centering
\begin{tikzpicture}
  \pie[text=inside,hide number,radius=2]{30/$L(M)$, 70/}
  \pie[text=inside,hide number,radius=2,pos={6,0}]{10/$q_0$, 20/$q_1$, 30/$q_2$, 40/$q_3$}
  \pie[text=inside,hide number,radius=2,pos={12,0}]{30/$[x]$, 30/$[y]$, 40/$[z]$}
\end{tikzpicture}
\caption{(left) a partition of $\Sigma^\ast$ by a DFA $M$; (middle) a
partition of $\Sigma^\ast$ by the states of $M$; (right)` a partition of
$\Sigma^\ast$ by equivalence classes with respect to $L$.}
\label{fig:fig29}
\end{figure}

For a DFA acceptable language, namely a regular language, $\Sigma^\ast$
must be able to be partitioned into a finite number of classes with
respect to $\equiv_L$. The number of the classes is at most $|Q_M|$.
For a language $L$, if $\Sigma^\ast$ can be partitioned into infinitely
many classes with respect to $\equiv_M$, it does not seem to be
acceptable by a DFA.

\begin{theorem}[Myhill-Nerode Theorem]
Fix the alphebet $\Sigma$. For a language $L$, if $\Sigma^\ast$ has
infinitely many equivalence classes with respect to $\equiv_L$, then
$L$ is not regular. Otherwise, $L$ is regular and is accepted by a
DFA $M$ where $|Q_M|$ is equal to the number of equivalence classes.
\end{theorem}

\begin{proof}
If $\Sigma^\ast$ has infinitely many equivalence classes with respect to
$L$, we draw a string from each of the classes. For any pair $(x,y)$ of
them, $x \equiv_L y$ does not hold, that is to say we need a distinct
state for them. That need infinitely many states in a DFA. Thus
impossible.
\end{proof}

For example, consider the language $L = \{a^nb^n \mid n \in \mathbb{N}\}$.
For $L$, all strings $a^n$ where $n \in \mathbb{N}$ is distinguishable
pairwisely. That is to say that there are infinitely many equivalence
classes respect to $\equiv_L$, so $L$ is not a regular language.

\subsection{Pumping Lemma}

Since there are finite states in a DFA, by the Pigeonhole Principle, if
fed with a sufficiently large string, there must be a state that is
visited twice. For a state to be visited twice, as we can imagine, there
must be a \emph{loop} containing the state.

Figure \ref{fig:fig30} shows an illustration of this situation. A long
string, with some prefix $x$, enters the loop and then goes through some
suffix and finally gets accepted.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \node[state,initial] (q0) {$q_0$};
      \node[state,right of=q0] (q1) {$q_1$};
      \node[state,accepting,right of=q2] (q2) {$q_2$};

      \draw (q0) edge[above] node{$x$} (q1)
            (q1) edge[loop,above] node{$y$} (q1)
            (q1) edge[above] node{$z$} (q2);
    \end{tikzpicture}
    \caption{minimized version of figure \ref{fig:fig27}}
    \label{fig:fig30}
\end{figure}

The interesting part is that as there is this loop yielding "y", it can
yield more or less (to $0$ times of "y" which is $\epsilon$) of "y"s, and
the new strings are still accepted by $M$. This phenomena is stated
formally in the following theorem:
\begin{theorem}[Pumping Lemma]
Let $L$ be a regular language. Then there exists a number $p$ such that \\
$\forall s \in L$, $|s| \geq p$, \\
\indent $\exists xyz$, $s = xyz$, $|y| > 1$, \\
\indent\indent $\forall i \geq 0$, $xy^iz \in L$.
\end{theorem}

For example, consider the language $L = \{a^nb^n \mid n \in \mathbb{N}\}$.
Assume $L$ is a regular language. For a number $p$ guaranteed by the Pumping
Lemma, consider a string $a^pb^p$. There must be a nonempty substring $y$ in
it, where we can pump it up or down and the resultant strings are still in
$L$. There are three possible cases:
\begin{enumerate}
\item $y = a^i$ where $i \leq p$.
\item $y = a^ib^j$ where $i, j \leq p$
\item $y = b^j$ where $j \leq p$
\end{enumerate}

In any of these cases, either we pump up or pump down $y$ would cause a
mismatch between the count of "a"s and the count of "b"s in the resultant
string. This contradicts with Pumping Lemma. Thus $L$ is not a regular
language.

Actually, Pumping Lemma gives us a tighter condition about where $y$ could
be located within the string. In the example above, we are working on a
string $a^pb^p$ whose length is twice as the $p$ guaranteed by Pumping
Lemma. There must be \emph{a} $y$ within the first $p$ characters. And we
could just drop the last two cases and the analysis would be simpler.

\section{Context Free Grammar}

Let us move on to another system to specify a language. We show an example
here:
\begin{align*}
&S \rightarrow aSb \\
&S \rightarrow \epsilon
\end{align*}

These two lines form a \emph{rule}, and each of the lines is called a
production. The capital letter $S$ denotes a \emph{variable}. The lowercase
letters denote \emph{terminals}. Only variables can appear on the left hand
side of the equation.

One possible process this rule can generate a string is shown below. This
line is also called a \emph{derivation}.

\begin{equation*}
\centering
S \rightarrow aSb \rightarrow aaSbb \rightarrow aa \epsilon bb \rightarrow aabb
\end{equation*}

To derive a string from a rule, we start from the \emph{starting point},
usually denoted by the variable $S$. Then we pick a production where $S$ appears
on the left hand side, substitute $S$ with the right hand side of the
production. We can repeat this substitution indefinitely until there is no
nonterminals left. Then we get a string from it.

The example above represents the language $\{a^nb^n \mid n \in \mathbb{N}\}$.
This language was proven nonregular by us in many ways. We are now entering
the new realm of nonregular languages.

\section{Context Free Grammar. Cont}

\subsection{Derivation, Parse Tree and Ambiguity}

Consider the example:

\begin{align*}
&S \rightarrow aAbB \\
&A \rightarrow \epsilon | aA \\
&B \rightarrow \epsilon | bB
\end{align*}

There are two ways to derive the string $aabb$ from these rules:

\begin{enumerate}
\item $S \rightarrow aAbB \rightarrow aaAbB \rightarrow aabB \rightarrow aabbB \rightarrow aabb$
\item $S \rightarrow aAbB \rightarrow aAbbB \rightarrow aAbb \rightarrow aaAbb \rightarrow aabb$
\end{enumerate}

In the first way, we always expand the leftmost variable first. While in the second way, we always
expand the rightmost variable first. Note that those are not only ways to derive $aabb$. But since
they are special, we give them names \emph{leftmost derivation} and \emph{rightmost derivation}.

Alternatively, we can draw a \emph{parse tree} of $aabb$. Though there are multiple possible
derivations, there is only one parse tree for $aabb$.

\begin{figure}[ht]

\Tree
[.S
  [.a ]
  [.A
    [.a ]
    [.A
      [.$\epsilon$ ]
    ]
  ]
  [.b ]
  [.B
    [.b ]
    [.B
      [.$\epsilon$ ]
    ]
  ]
]

\caption{the parse tree for the example}
\label{fig:fig31}
\end{figure}

Unfortunately, this is not always the case. It is possible that there are multiple distinct
parse trees for a string. For example consider this CFG:

\begin{align*}
&S \rightarrow aAb \\
&A \rightarrow aA|ab|b
\end{align*}

And for the string $aabb$, there can be two distinct parse trees for it:

\begin{figure}[ht]

\Tree
[.S
  [.a ]
  [.A
    [.ab ]
  ]
  [.b ]
]
\Tree
[.S
  [.a ]
  [.A
    [.a ]
    [.A
      [.b ]
    ]
  ]
  [.b ]
]


\caption{two possible parse trees for the example}
\label{fig:fig32}
\end{figure}

In the case that a string $s$ has more than one possible parse tree using the rules in a grammar
$G$, we say that the string $s$ is \emph{ambiguous} with respect to $G$. And a grammar $G$ is
ambiguous if there exists a string $s$ that is ambiguous with respect to $G$. There are methods
to eliminate ambiguity for some $G$. But there are some grammars that are inherently ambiguous
and no method can eliminate the ambiguity.

\subsection{Formal Definitions for Concepts}

Now we can give a formal definition to a \emph{context free grammar}. There are also formal
definitions of \emph{yield} and \emph{derive}.

\begin{definition}
A context free grammar, $G$, is a tuple $(V,\Sigma,R,S)$ where
\begin{itemize}
\item $V$ is a finite set of \emph{variables}
\item $\Sigma$ is the alphabet
\item $R \subseteq V \times (V \cup \Sigma)^\ast$ is a finite set of \emph{rules}
\item $S \in V$ is the \emph{start variable}
\end{itemize}
\end{definition}

\begin{definition}
Suppose $x$, $y$ and $w$ are strings in $(V \cup \Sigma)^\ast$ and $B$ is a variable. Then
$xBy$ \emph{yields} $xwy$, written as $xBy \Rightarrow xwy$, if there is a rule in $R$ of the form
$B \rightarrow w$.
\end{definition}

\begin{definition}
If $x$ and $w$ in $(V \cup \Sigma)^\ast$, then $w$ \emph{derives} $x$, written as $w \xRightarrow
{\ast} x$, if we can get from $w$ to x in zero or more yields steps.
\end{definition}

\begin{definition}
If $G$ is a grammar, then $L(G)$, the language of $G$, is the set $\{w \in \Sigma^\ast \mid
S \xRightarrow{\ast} w\}$.
\end{definition}

\subsection{DFAs and CFGs}

Given a DFA $M$, we can convert it to a CFG $G$ representing the same language easily. To do it,
we need to
\begin{enumerate}
\item make a variable $Q_i$ for each state $q_i$ of M
\item add the rule $Q_i \rightarrow aQ_j$ to $G$ if there is a transition with $a$ from $q_i$ to
$q_j$
\item add the rule $Q_i \rightarrow \epsilon$ if $q_i$ is an accept state
\item make $Q_0$ the start variable of $G$, where $q_0$ is the start state of $M$
\end{enumerate}

\section{Pushdown Automata}

\subsection{The Definition}

As we did for regular languages, for \emph{context free languages} we would like to build some
automata to accept them. It turns out that there is exactly a class of machines, called \emph{
pushdown automata}, accepting them.

We define a pushdown automaton formally as follows:

\begin{definition}
A \emph{pushdown automaton}, $M$, is a tuple $(Q,\Sigma,\Gamma,\delta,q_0,F)$ where
\begin{itemize}
\item $Q$ is a finite set of states
\item $\Sigma$ is the input alphabet
\item $\Gamma$ is the stack alphabet
\item
$\delta : Q \times \Sigma_{\epsilon} \times \Gamma_{\epsilon} \mapsto \mathcal{P}(Q \times \Gamma)$
is the transition function
\item $q_0 \in Q$ is the start state
\item $F \subseteq Q$ is the set of the accept states
\end{itemize}
\end{definition}

The definition above is quite similar to that of a NFA. What is new here is the presence of a
\emph{stack}. There is the alphabet $\Gamma$ associated with the stack, and the $\delta$ function
reads an extra input which is a symbol drawn from $\Gamma_{\epsilon}$ and produces an extra
output to the stack. As the definition of the $delta$ function suggests, since there are
$\epsilon$ subscripts in the input alphabets and the output is a set, a PDA is nondeterministic.

Now, we describe how a PDA $M$ computes. Basically, it reads a symbol from the input string and
a symbol from the stack, transitions to the next state and puts a symbol to the stack following
$\delta$. Just like DFAs/NFAs, it reads the input string from left to right and can never read
backward. The key difference with DFA/NFA is that there is a stack of infinite capacity. Without
it a PDA would be nothing but a DFA/NFA.

More formally, for a PDA $M$, a \emph{configuration} of $M$ is a tuple $(q,string,stack)$. Assume
the current configuration of $M$ is $(q,w \cdot w_{\text{rest}},s \cdot s_{\text{rest}})$. On
reading an input symbol $w$, if there is a $(q',s') \in \delta(q,w,s)$, $M$ transitions to the
new configuration $(q',w_{\text{rest}},s' \cdot s_{\text{rest}})$. There may be many pairs
$(q,s)$ produced by the $\delta$ function, and it behaves pretty much like the way a NFA behaves.
It splits out for each pair produced. There may be no pair at all, then the (current branch of)
computation terminates immediately.

What we described above is a single step of computation of a PDA. For a DFA/NFA, we defined a
$\delta^\ast$ function specifying how it transitions on a string rather than a symbol. We can
do the same for a PDA. We denote the single step computation from configuration $c_1$ to
configuration $c_2$ by $c_1 \vdash c_2$. And we denote $c_1 \vdash^\ast c_n$ if there exists
finite steps of computations $c_1 \vdash c_2$, $c_2 \vdash c_3$, $\dots$, $c_{n-1} \vdash c_n$.

Now, we can write down whether a PDA $M$ accepts a string $w$. That is, $M$ accepts $w$ if and only
if $(q_0,w,\epsilon) \vdash^\ast (q_f,\epsilon,s)$ where $q_f \in F$.

\subsection{The Equivalence}

\begin{theorem}
A language is context free if and only if some PDA accepts it.
\end{theorem}

Suppose $L = L(G)$ for some CFG $G$. To build a PDA $M$ accepting $L$:

\begin{enumerate}

\item We must be able to test if the stack is empty. The way to do it is that we put a
special purpose symbol, say $\$$, right after the PDA starts. That is $\delta(q_0,\epsilon,\epsilon) = \{(q_1,\$)\}$.

\item Then, we put the start variable of $G$, $S$. That is
$\delta(q_1,\epsilon,\epsilon) = \{(q_2,S)\}$.

\item The next move of $M$ depends on the stack. If the top of the stack is a variable symbol $A$,
select one of the rules for $S$ and substitutes $A$ by the string on the right hand side of the
rule.

\item If the top of the stack is a terminal symbol $a$, read the next input symbol and compare it
to $a$. If they match, transition to the next configuration following $\delta$. If not, terminate
this branch of computation.

\item If the top of the stack is the marker $\$$, transition to the accept state.

\item Repeat step 3-5 until all branches of the computations of $M$ are terminated or the input
string is exhausted.

\end{enumerate}

\section{CNF, CYK Parsing and Pumping Lemma for CFL}

\subsection{Chomsky Normal Form}

CFGs can be messy. For example, consider the grammar shown below:

\begin{align*}
&S_0 \rightarrow S|X|Z \\
&S \rightarrow A \\
&A \rightarrow B \\
&B \rightarrow C \\
&C \rightarrow Aa \\
&X \rightarrow C \\
&Y \rightarrow aY|a \\
&Z \rightarrow \epsilon
\end{align*}

There are several problems with this grammar. Variable $Y$ is unreachable from $S_0$ so
the rule for it is useless. $A$ is useless too, because as we reach $A$ we immediately get trapped
in an infinite loop. Rules like $S \rightarrow A$ are redundant in the sense that we can replace
any occurrence of $S$ by $A$. There rules are called \emph{unit rules}.

We can resolve these problems by removing useless variables and rules involving them. What's more,
there is a normal form for CFG call \emph{Chomsky normal form}, abbreviated CNF, defined as
follows:

\begin{definition}
A context free grammar is in \emph{Chomsky normal form} if every rule is of the form
\begin{align*}
&A \rightarrow BC \\
&A \rightarrow a
\end{align*}
where $a$ is any terminal and $A$, $B$ and $C$ are variables except that $B$ and $C$ may not be
the start variable. In addition, we permit $S \rightarrow \epsilon$.
\end{definition}

We can transform any context free grammar $G$ into Chomsky normal form. First, we add a new start
variable $S_0$ and a rule $S_0 \rightarrow S$ (which guarantees that the start variable does not
appear at the right hand side of a rule). Then, we eliminate all $\epsilon$ rules. We also
eliminate unit rules. Finally, we convert the remaining rules into the proper form.

To eliminate a $\epsilon$ rule $A \rightarrow \epsilon$, for each occurrence of $A$ on the right
hand side of a rule, we add a new rule with that occurrence removed. For example, $C \rightarrow
aAbAc$ becomes three rules $C \rightarrow abAc$, $C \rightarrow aAbc$ and $C \rightarrow abc$. If
we have $C \rightarrow A$, we add a rule $C \rightarrow \epsilon$ (only if we did not eliminate
$C \rightarrow \epsilon$). After this, we can safely remove the rule $A \rightarrow \epsilon$.

To eliminate a unit rule $A \rightarrow B$, for each rule $B \rightarrow u$, where $u$ is a string
of variables and terminals, we add a rule $A \rightarrow u$ (only if this rule is not a unit rule
already removed). After this, we can safely remove the rule $A \rightarrow B$.

Finally, we cleanup the remaining rules. We break all rules whose right hand side is of length
greater than 2 or is a mixture of terminals and variables into smaller rules by adding several
intermediate variables.

\subsection{Cocke-Younger-Kasami Parsing}

We want to test if a string can be derived from a CFG. For a string $s$ whose length is $n$, we
reconstruct the parse tree bottom-up. In the first iteration, we break $s$ into $n$ symbols and
check for each symbol which rule can derive it. In the second iteration, we group adjacent symbols
into strings of length $2$, and check for each such string which rule can derive it. In the next
iteration, as we can imagine, we need to parse strings of length $3$. As this procedure continues,
at some point, we will need to parse a string of length $n$, which is the original string $s$.

\subsection{Pumping Lemma}

A DFA has a finite number of states and a CFA has a finite number of variables. Any sufficient long
string drives the DFA visiting a state at least twice. In a similar manner, instead of a
computation in the case of a DFA, a parse tree of any sufficient long string with respect to a CFG
must have a path from the root to a leaf consisting of at least one variable duplicated.

Say, there is a path $S \dots R \dots R \dots a$, where $S$ is the start variable, $R$ is a
variable occurred twice and $a$ is a terminal. Note that there can be multiple paths like this with
(maybe distinct) $a$s. That is to say, the second occurrence of $R$ yields a substring $x$ and
the first occurrence of $R$ yields a substring $vxy$. This suggests $R$ can yield a string
containing some string yielded by $R$ - the $x$ part. For $vxy$ yielded by $R$, since $x$ is also
yielded by $R$, we can replace $x$ with another $vx'y$ where $x'$ can be yielded by $R$. This is
quite similar to the Pumping lemma for DFA.

\begin{theorem}[Pumping Lemma for CFL]
For any context free language $L$, there is a number $p$ (the pumping length) such that \\
$\forall s \in L, |s| \geq p$, \\
\indent $\exists uvxyz$, $s = uvxyz$, $|uv| > 0$ and $|vxy| \leq p$, \\
\indent\indent $\forall i \geq 0$, $uv^ixy^iz \in L$.
\end{theorem}


\section{Turing Machine}

We move on to a more powerful class of machines, \emph{Turing Machine}. Recall that a finite
automaton can be viewed as a finite control reading an input string from left to right and can never
read backward. A pushdown automata is a finite automaton with a stack of infinite size, but the
stack can only be used in a restricted way. That is, we can only read the top of the stack, and can
only write to the top of the stack.

A Turing machine is a finite control with an infinite tape. There is a \emph{head} pointing to the
current character. At the beginning, the head points to the first character on the tape. Then the
machine starts computing. It reads the symbol under the head, write a new character under the head
and then move the head either to the left or to the right. It is allowed that it does only the read
and does not write, or it does not move the head. There are states in the finite control. As the
computation going, it transitions from one state to another and hence performing different
operations on same input. There are two special states: the \emph{accept state} and the
\emph{reject state}. If the machine enters either of the accept state or the reject state, it
immediately terminates, or \emph{halts}. We say the machine \emph{accepts} or \emph{rejects}
depending on which state it enters. It is possible that for some inputs the machine never halts. It
will keep running forever.

Formally, a Turing machine is defined as follows:
\begin{definition}
A Turing machine, $M$, is a tuple $(Q,\Sigma,\Gamma,\delta,q_0,q_a,q_r)$ where
\begin{itemize}
\item $Q$ is a finite set of states
\item $\Sigma$ is the input alphabet not containing the \emph{blank symbol \textvisiblespace}
\item $\Gamma$ is the tape alphabet, where $\text{\textvisiblespace} \in \Gamma$ and $\Sigma
\subseteq \Gamma$
\item $\delta : Q \times \Gamma \mapsto Q \times \Gamma \times \{L,R\}$ is the transition function
\item $q_0 \in Q$ is the start state
\item $q_a \in Q$ is the accept state
\item $q_r \in Q$ is the reject state, where $q_a \neq q_r$
\end{itemize}
\end{definition}

To formalize how a Turing machine computes, we first formalize how to denote the state of it.

A snapshot of the current state of a Turing machine consists of three components, the current state,
the current tape contents and the position where the head points to. We call this tuple a
\emph{configuration}. For a state $q$ and two strings $u$ and $v$, the configuration is written as
$uqv$ indicating that the current state is $q$, the current tape contents is $uv$ and the current
head position is at the first character of $v$.

For two configurations $C_1$ and $C_2$, we say $C_1$ \emph{yields} $C_2$ if the Turing machine can
go from $C_1$ to $C_2$ in one step. Suppose we have $a$, $b$ and $c$ in $\Gamma$, and $u$ and $v$
in $\Gamma^\ast$ and $q_i$ and $q_j$ in $Q$. There are several cases possible:
\begin{enumerate}
\item $uaq_ibv$ yields $uq_jacv$ if $\delta(q_i,b)=(q_j,c,L)$
\item $uaq_ibv$ yields $uacq_jv$ if $\delta(q_i,b)=(q_j,c,R)$
\end{enumerate}

For a special case when we want to move to the left on the leftmost position, the head just remains
on the same position. That is $q_iav$ yields $q_jcv$ if $\delta(q_i,a)=(q_j,c,L)$. If the head is
on the rightmost position, however, note that $uaq_i$ is equivalent to
$uaq_i\text{\textvisiblespace}$ and there is nothing special to handle it.

As we did for finite automata and pushdown automata, we define a reflexive and transitive closure
of the yield relation. We say that $C_i$ \emph{derives} $C_j$ if it takes finite number steps from
$C_i$ to yield $C_j$. And according the our definition above, the Turing machine halts immediately
once it enters the accept state or the reject state. So for a Turing machine $M$ and a string $s$,
if the start configuration $C_0$ derives the accept configuration $C_a$ on $s$, it \emph{accepts}
$s$; if $C_0$ derives the reject configuration $C_r$, it \emph{rejects} $s$.

\begin{definition}
A language if \emph{Turing recognizable} if there is a Turing machine $M$ recognizes it. We write
$L(M) = L$.
\end{definition}

\begin{definition}
A language $L$ is recursively enumerable if there exists a Turing machine $M$ such that $L(M) = L$.
\end{definition}

Note that if a Turing machine $M$ recognizes a language $L$, it means that $M$ halts for all
$s \in L$ in an accept configuration. But for a $s \not\in L$, $M$ may halt in a reject
configuration or loop forever. Sometimes we want a stronger condition that $M$ does halt in a reject
configuration for $s \not\in L$. In this case we call $M$ \emph{decides} $s$. And for a language we
define:

\begin{definition}
A language is \emph{Turing decidable} if there is a Turing machine decides it.
\end{definition}

\section{Variants of Turing Machines}

There are numerous alternative definitions of Turing machines and each of them has the same power
with the ordinary Turing machines we defined in last section. For example, for a transition in a
Turing machine, the head could just stay put rather than moving to the left or right.

\subsection{Multitape Turing Machines}

A \emph{multitape Turing machine} is like an ordinary Turing machine with several tapes. Each tape
has its own head for reading and writing. The transition function is like
\[
  \delta: Q \times \Gamma^k \mapsto Q \times \Gamma^k \times \{L,R,S\}^k
\]

\subsection{Nondeterministic Turing Machines}

As we did for finite automata and pushdown automata, Turing machines can also be extended to have
nondeterministic behaviours. To be specific, the transition function of a nondeterministic Turing
machine is like
\[
  \delta: Q \times \Gamma \mapsto \mathcal{P}(Q \times \Gamma \times \{L,R\})
\]

\subsection{Church-Turing Thesis}

The \emph{Church-Turing Thesis} says that our intuitive notion of algorithms (computations) is equal
to that of a Turing machine.

\subsection{Encoding Problems}

Problems can be encoded into strings. For example, we want to know whether a polynomial of one
variable $x$ has an integer root. We can encode the polynomial $p$ into a string $\langle p
\rangle$ and feed it into a Turing machine $M$. Then $M$ enumerates integers in the order $0,1,-1,
2,-2,\dots$, substitute $x$ with the integer and test if the polynomial equals to $0$. If it does,
$M$ accepts the string $\langle p \rangle$, which means that $p$ has an integer root. So the
language $L = \{\langle p \rangle \mid p \text{ is a polynomial over } x \text{ with an integer
root}\}$ is Turing-recognizable. In this case, without extra knowledge, the algorithm given to test
whether a polynomial over $x$ has an integer root is recognizable but not decidable.

\section{Decidable and Undecidable Problems}

In this section we give some examples on decidability.

Let us begin with a simple decidable problem. For a DFA $D$ and a string $w$, is there a Turing
machine deciding whether $D$ accepts $w$?

\begin{theorem}
Let $A_{DFA} = \{\langle D,w \rangle \mid D \text{ is a DFA that accepts } w\}$. $A_{DFA}$ is
decidable.
\end{theorem}

\begin{proof}
We construct a Turing machine $M$ working as follows. On input $\langle D,w \rangle$, $M$ simulates
$M$ on $w$. $M$ accepts $\langle D,w \rangle$ if and only if $D$ accepts $w$. As DFAs are deciders,
$M$ is a decider.
\end{proof}

And with no surprise, there is also a Turing machine which is a decider for CFGs:

\begin{theorem}
let $A_{CFG} = \{\langle G \rangle \mid G \text{ is a context free grammar that generates } w\}$.
$A_{CFG}$ is decidable.
\end{theorem}

\begin{theorem}
Let $A_{TM} = \{\langle M,w \rangle \mid M \text{ is a Turing machine and } M \text{ accepts } w\}$.
$A_{TM}$ is undecidable.
\end{theorem}

\section{Reducibility}

\subsection{Many-One Reduction}

\begin{definition}
For two languages $A$ and $B$, we say that $A$ \emph{many-one reduces} to $B$, denoted
$A \leq_m B$, iff there exists a Turing-computable function $f$ such that
$x \in A \Leftrightarrow f(x) \in B$.
\end{definition}

\begin{proposition}
Suppose $A$ many-one reduces to $B$. If $B$ is decidable, $A$ is also decidable.
\end{proposition}

\begin{proof}
If $B$ is decidable then there is a decider $M_B$ for $B$. For any string $x \in A$, we compute
$f(x) \in B$ and run $M_B$ to decide if $f(x)$ is in $B$. And by the definition of many-one
reduction, if $f(x) \in B$, then $x \in A$. If $f(x) \not\in B$, then $x \not\in A$. That is to say
we have a decider for $A$.
\end{proof}

\begin{corollary}
Suppose $A$ many-one reduces to $B$. If $A$ is not decidable, $B$ is also not decidable.
\end{corollary}

We can use this corollary to show that a language is undecidable. We already know a undecidable
language $A_{TM}$. If $A_{TM}$ many-one reduces to $L$, then $L$ must be undecidable.

\begin{corollary}
Suppose $A$ many-one reduces to $B$. If $B$ is Turing-recognizable, $A$ is also Turing-recognizable.
\end{corollary}

\begin{corollary}
Suppose $A$ many-one reduces to $B$. If $A$ is not Turing-recognizable, $B$ is also not
Turing-recognizable.
\end{corollary}

\begin{corollary}
Suppose $A$ many-one reduces to $B$. If $B$ is co-Turing-recognizable, $A$ is also
co-Turing-recognizable.
\end{corollary}

\begin{corollary} \label{corollary:43}
Suppose $A$ many-one reduces to $B$. If $A$ is not co-Turing-recognizable, $B$ is also not
co-Turing-recognizable.
\end{corollary}

One way to remember this is that as the notation $\leq_m$ suggests, if $A \leq_m B$, $B$ is a
problem harder than $A$. It is at least as hard as $A$. To solve $A$, we need to first solve $B$.
And if the harder problem can be solved, so does the simpler one. If we cannot even solve the
simpler problem, then surely we cannot solve the harder one.

\subsection{An Example: The Halting Problem}

Consider this language
\[
  BTHP = \{\langle M \rangle \mid M \text{ is a TM and } M \text{ halts on blank tape}\}
\]

We claim that this $BTHP$ is undecidable. First, we note that $BTHP$ is Turing-recognizable. For a
TM $M_{BTHP}$, it simulates the input TM $M$ on a blank tape and see if it halts. $M_{BTHP}$ accepts
if $M$ halts. So to show that $BTHP$ is undecidable, we need to show that it is not
co-Turing-recognizable. By corollary \ref{corollary:43}, if we can find a language $A$ that is not
co-Turing-recognizable and find a many-one reduction from $A$ to $BTHP$, then we would have shown
that $BTHP$ is not co-Turing-recognizable.

Since $A_{TM}$ is Turing-recognizable and undecidable, it is not co-Turing-recognizable. Let us try
to see if we can many-one reduce $A_{TM}$ to $BTHP$. $M_{TM}$ is such a TM that
\begin{itemize}
\item accepts if $M$ accepts $w$
\item rejects if $M$ rejects $w$
\end{itemize}

Can we build a TM $M_{BTHP}$ that helps us to compute $M_{TM}$? That is to say that, can we build
such a machine $M_{BTHP}$ that accepts if $M'$ halts on the blank tape, and $M'$ has the following
properties:
\begin{itemize}
\item $M'$ halts on the blank tape if $M$ accepts $w$
\item $M'$ loops if $M$ rejects $w$
\end{itemize}

There is a way to build $M'$ out from $M$ and $w$. For any input $k$, $M'$ just simulates $M$ on $w$
and accepts if $M$ accepts $w$. Note that this simulation does not depend on $k$ and, in fact, does
not use $k$ at all. So $M'$ accepts whatever $k$, including the blank tape, if $M$ accepts $w$. And
$M'$ loops if $M$ rejects $w$. Although there is no way for $M'$ to test if $M$ loops on $w$, if
$M$ does loop on $w$, $M'$ loops as well. So this construction of $M'$ works.

Since we can many-one reduce $A_{TM}$ to $BTHP$, by corollary \ref{corollary:43}, we know that
$BTHP$ is not co-Turing-recognizable. Combined with the fact that $BTHP$ is Turing-recognizable,
we know that $BTHP$ is undecidable.

\subsection{An Example: FINITE}

Consider this language
\[
  FINITE = \{\langle M \rangle \mid M \text{ is a TM and } L(M) \text{ is finite}\}
\]

It seems that $FINITE$ is neither Turing-recognizable nor co-Turing-recognizable. To show that it is
not Turing-recognizable, we need to find a many-one reduction from $\bar{A}_{TM}$ to it. To show
that it is not co-Turing-recognizable, we need to find a many-one reduction from $A_{TM}$ to it.

\subsubsection{First Part: Non-Turing-Recognizable}

Let us show that $\bar{A}_{TM}$ many-one reduces to $FINITE$. To be specific, we want to build a TM
$M_{FINITE}$ that helps us to compute $\bar{M}_{TM}$. That is to say that, we want to build a
machine $M_{FINITE}$ that accepts if $L(M')$ is finite and $M'$ has the following properties
\begin{itemize}
\item $L(M')$ is finite if $M$ does not accept $w$
\item $L(M')$ is infinite if $M$ accepts $w$
\end{itemize}

There is a way to build $M'$ out from $M$ and $w$. For any input $x$, $M'$ simulates $M$ on $w$ and
accepts if $M$ accepts $w$. If $M$ rejects $w$, $M'$ rejects. That is, if $M$ accepts $w$, $M'$
accepts all strings and thus infinite. If $M$ does not accept $w$, $M'$ either rejects or loops and
in this case, it does not accept any string and thus $L(M) = \emptyset$.

\subsubsection{Second Part: Non-Co-Turing-Recognizable}

Let us show that $A_{TM}$ many-one reduces to $FINITE$. This time we want $M'$ to have the following
properties:
\begin{itemize}
\item $L(M')$ is finite if $M$ accepts $w$
\item $L(M')$ is infinite if $M$ does not accept $w$
\end{itemize}

There is a way to build $M'$ out from $M$. For any input $k$, run $M$ on $w$ for $|k|$ steps, and
\begin{itemize}
\item reject if $M$ has accepted by now
\item accept if $M$ has rejected by now
\item accept if $M$ has neither accepted nor rejected by now
\end{itemize}

To see why this construction works, let us first check for the case in which $M$ accepts $w$. Assume
$M$ accepts $w$ on the $x$th step. For any input $k$ of $M'$ where $|k| \leq x$, $M'$ accepts $k$.
For $|k| > x$, $M'$ rejects $k$. This is to say that if $M$ accepts $w$, $M'$ accepts each string
whose length is less than $x$. $L(M')$ is finite. If $M$ does not accept $w$, then $M'$ would accept
every input $k$. In this case $L(M')$ is infinite.

\section{More on Reduction}

\subsection{Languages That Are Neither Recognizable Nor Co-Recognizable}

We have seen examples of Turing-recognizable languages and co-Turing-recognizable languages. It seems
that Turing machine is so powerful that any language may be either Turing-recognizable or
co-Turing-recognizable. In fact, there are many languages that are neither Turing-recognizable
nor co-Turing-recognizable. We can count how many such languages are there.

First, note that there are countably infinite Turing-recognizable languages, and countably infinite
co-Turing-recognizable languages. The reason is that Turing-machines, which are able to be encoded
by finite strings, are countable. But the set of all languages, $\mathcal{P}(\Sigma^\ast)$ is
uncountable. So there are uncountably languages that are neither Turing-recognizable nor
co-Turing-recognizable.

\subsection{An Example Concerning CFL}

Consider the following language
\[
  L = \{\langle M \rangle \mid L(M) \text{ is a CFL}\}
\]

We claim that $L$ is neither Turing-recognizable nor co-Turing-recognizable. Let us prove it.

\subsubsection{First Part: Non-Co-Turing-Recognizable}

Let us show that $A_{TM}$ many-one reduces to $L$. We want to build a TM $M_L$ that helps us to
compute $A_{TM}$. That is to say, $M_L$ accepts if $L(M')$ is a CFL and $M'$ has the following
properties:
\begin{itemize}
\item $L$ is a CFL if $M$ accepts $w$
\item $L$ is not a CFL if $M$ does not $w$
\end{itemize}

There is a way to build $M'$ out from $M$ and $w$. First, fix a non-CF language such as
$N = \{a^nb^nc^n \mid n \in \mathbb{N}\}$. Then, we can build $M'$ such that
\begin{itemize}
\item for any input $x$, accept if $x \in N$
\item run $M$ on $w$, accept if $M$ accepts $w$
\item reject if $M$ rejects $w$
\end{itemize}

$M'$ accepts a non-CF language unconditionally. For input $x$ that is not in that non-CF
language, it simulates $M$ to decide whether or not $x$ should be accepted. If $M$ accepts
$w$, then every (other) $x$ is accepted, consequently $\Sigma^\ast$ is accepted by $M'$.
If $M$ does not accept $w$, no matter it rejects or loops, no additional strings are
accepted. Eventually $M'$ accepts only the initially accepted non-CF language.

\subsubsection{Second Part: Non-Turing-Recognizable}

Let us show that $\bar{A}_{TM}$ many-one reduces to $L$. That is to say we want to build $M'$ such
that
\begin{itemize}
\item $L$ is a CFL if $M$ does not accept $w$
\item $L$ is not a CFL if $M$ accepts $w$
\end{itemize}

There is a way to build $M'$ out from $M$ and $w$. For any input $x$
\begin{itemize}
\item accept if $M$ accepts $w$ and $x \in \{a^nb^nc^n \mid n \in \mathbb{N}\}$
\item reject if $M$ rejects
\end{itemize}

\subsection{Rice's Theorem}

\begin{theorem}
Let $P$ be any nontrivial property of the language of a Turing machine and \\
$L_P = \{\langle M \rangle \mid M \text{ is a Turing machine and } P(L(M))\}$. $L_P$ is
undecidable.
\end{theorem}

\section{P and NP}

\subsection{Running Time of Turing Machine}

\begin{definition}
Let $M$ be a deterministic Turing machine that halts on all inputs. The \emph{running time} or
\emph{time complexity} is the function $f: \mathbb{N} \mapsto \mathbb{N}$, where $f(n)$ is the
maximum number of steps that $M$ uses on any input of length $n$.
\end{definition}

\begin{definition}
Let $t: \mathbb{N} \mapsto \mathbb{R}^+$ be a function. Define the \emph{time complexity class},
$TIME(t(n))$, to be the collection of all languages that are decidable by an $O(t(n))$ time Turing
machine.
\end{definition}

\begin{definition}
Let $N$ be a nondeterministic Turing machine that is a decider. The \emph{running time} of $N$ is
is the function $f: \mathbb{N} \mapsto \mathbb{N}$, where $f(n)$ is the maximum number of steps
that $N$ uses on any branch of its computation on any input of length $n$.
\end{definition}

\subsection{The Class P}

\begin{definition}
\textbf{P} is the class of languages that are decidable in polynomial time on a deterministic
single-tape Turing machine. In other words,
\[
  P = \bigcup_k TIME(n^k).
\]
\end{definition}

\subsection{The Class NP}

\begin{definition}
The \emph{verifier} of a language $A$ is a algorithm $V$, where
\[
  A = \{w \mid V \text{ accepts }\langle w,c \rangle \text{ for some string }c\}.
\]
We measure the time of a verifier only in terms of the length of $w$, so a \emph{polynomial time
verifier} runs in polynomial time in the length of $w$. A language $A$ is \emph{polynomially
verifiable} if it has a polynomial time verfier.
\end{definition}

\begin{definition}
The class \textbf{NP} is the class of languages that have polynomial time verifiers.
\end{definition}

\begin{definition}
$\mathrm{NTIME}(t(n))$ is the set \\ $\{L \mid L \text{ is a language that is decidable by an }
O(t(n)) \text{ time nondeterministic Turing machine}\}$.
\end{definition}

\begin{corollary}
$\textbf{NP} = \bigcup_k \mathrm{NTIME}(n^k)$.
\end{corollary}

\subsection{N Versus NP}

The question of whether $P = NP$ is one of the greatest unsolved problems in the theoretical
computer science and contemporary mathematics.

\subsection{NP Completeness}

\begin{definition}
A function $f: \Sigma^\ast \mapsto \Sigma^\ast$ is a \emph{polynomial time computable function} if
some polynomial time Turing machine exists that halts with just $f(w)$ on the tape, when started
with any input $w$.
\end{definition}

\begin{definition}
A language $A$ is \emph{polynomial time many-one reducible to} language $B$, writte $A \leq_P B$,
if a polynomial time computable function $f$ exists, where for every $w$,
\[
  w \in A \Leftrightarrow f(w) \in B.
\]
\end{definition}

\begin{definition}
A language $B$ is \emph{NP-complete} if it satisfies two conditions:
\begin{enumerate}
\item $B$ is in NP, and
\item every $A$ in NP is polynomial time reducible to $B$
\end{enumerate}
\end{definition}

\end{document}
